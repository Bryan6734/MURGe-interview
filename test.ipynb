{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bryan Sukidi\\Projects\\interview_package_murge_ambiguity_project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from typing import List \n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"`resume_download` is deprecated and will be removed in version 1.0.0.\")\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,\n",
    "                 model_name) -> None:\n",
    "\n",
    "        # Utilize my GPU (RTX 3060)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "        # Initialize model and tokenizer\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "\n",
    "        # Suppress warnings\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "    def generate_response(self, prompt: str) -> str:\n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Generate model output ids\n",
    "        output = self.model.generate(**inputs, return_dict_in_generate=True, max_new_tokens=10)\n",
    "        output_token_ids = output.sequences[0]\n",
    "\n",
    "        # Decode output\n",
    "        decoded_output = self.tokenizer.decode(output_token_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Remove prompt from generated text (could be done by masking or indexing based off prompt string)\n",
    "        generated_text = decoded_output[len(prompt):].strip()\n",
    "\n",
    "        # Obtain action by manipulating the generated text (mistral outputs action in the form of \"Action: > action\")\n",
    "        generated_text = generated_text.split('\\n')[0][2:]\n",
    "\n",
    "        # Print prompt and action for debugging\n",
    "        print(\"[PROMPT] \" + prompt.replace(\"Action:\", \"\")) # replaced \"action\" solely for aesthetics\n",
    "        print(\"[ACTION] \" + generated_text)\n",
    "\n",
    "        return generated_text\n",
    "    \n",
    "    def generate_constrained_response(self, prompt: str, valid_actions: List[str]) -> str:\n",
    "\n",
    "        prompt = prompt.replace(\"Action:\", \"\") # Remove \"Action:\" from prompt\n",
    "\n",
    "        for action in valid_actions:\n",
    "\n",
    "            relevance_prompt = prompt + f\"\\nOn a scale of 1 to 10, that the action {action} is relevant to the long-term goal is: \"\n",
    "\n",
    "            relevance_score = self.obtain_relevance_score(relevance_prompt, action)\n",
    "\n",
    "            print(\"[PROMPT] \" + prompt)\n",
    "            print(\"[ACTION] \" + relevance_score)\n",
    "            \n",
    "\n",
    "    def obtain_relevance_score(self, prompt:str , action:str):\n",
    "        # \"Task-grounding\", according to authors\n",
    "\n",
    "        # Tokenize inputs\n",
    "        relevance_inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Generate model output ids\n",
    "        relevance_output = self.model.generate(**relevance_inputs, return_dict_in_generate=True, max_new_tokens=2)\n",
    "\n",
    "        # Decode output\n",
    "        relevance_decoded_output = self.tokenizer.decode(relevance_output.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "        return relevance_decoded_output\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # valid_action_scores = []\n",
    "        # modified_prompt = prompt.replace(\"Action:\", \"\")\n",
    "\n",
    "        # for action in valid_actions:\n",
    "        #     print(\"Looking at action: \", action)\n",
    "        #     # \"Task-grounding\", according to authors\n",
    "        #     relevance_prompt = modified_prompt + f\"\\nGiven the action {action}, what is the probability from 0 to 1 that this action is relevant in the long-term to our goal?\"\n",
    "\n",
    "        #     # \"World-grounding\", according to authors\n",
    "        #     affordance_prompt = modified_prompt + f\"\\nGiven the action {action}, what is the probability that this action is feasible in the current environment?\"\n",
    "\n",
    "        #     # Tokenize inputs\n",
    "        #     relevance_inputs = self.tokenizer(relevance_prompt, return_tensors=\"pt\").to(self.device)\n",
    "        #     affordance_inputs = self.tokenizer(affordance_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        #     # Generate model output ids\n",
    "        #     relevance_output = self.model.generate(**relevance_inputs, return_dict_in_generate=True, max_new_tokens=10)\n",
    "        #     affordance_output = self.model.generate(**affordance_inputs, return_dict_in_generate=True, max_new_tokens=10)\n",
    "\n",
    "        #     # Decode output\n",
    "        #     relevance_decoded_output = self.tokenizer.decode(relevance_output.sequences[0], skip_special_tokens=True)\n",
    "        #     affordance_decoded_output = self.tokenizer.decode(affordance_output.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 17.91s/it]\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROMPT] You are an intelligent robot. Your goal is to drop a knife in the living room. Knife is in the kitchen. You can navigate the environment, pick up items, and drop them. You are in the living room. You see: couch, television, book. You have the following items in your inventory: .\n",
      "[ACTION] You are an intelligent robot. Your goal is to drop a knife in the living room. Knife is in the kitchen. You can navigate the environment, pick up items, and drop them. You are in the living room. You see: couch, television, book. You have the following items in your inventory: .\n",
      "On a scale of 1 to 10, that the action go to bedroom is relevant to the long-term goal is: 1.\n",
      "[PROMPT] You are an intelligent robot. Your goal is to drop a knife in the living room. Knife is in the kitchen. You can navigate the environment, pick up items, and drop them. You are in the living room. You see: couch, television, book. You have the following items in your inventory: .\n",
      "[ACTION] You are an intelligent robot. Your goal is to drop a knife in the living room. Knife is in the kitchen. You can navigate the environment, pick up items, and drop them. You are in the living room. You see: couch, television, book. You have the following items in your inventory: .\n",
      "On a scale of 1 to 10, that the action go to kitchen is relevant to the long-term goal is: 10\n",
      "[PROMPT] You are an intelligent robot. Your goal is to drop a knife in the living room. Knife is in the kitchen. You can navigate the environment, pick up items, and drop them. You are in the living room. You see: couch, television, book. You have the following items in your inventory: .\n",
      "[ACTION] You are an intelligent robot. Your goal is to drop a knife in the living room. Knife is in the kitchen. You can navigate the environment, pick up items, and drop them. You are in the living room. You see: couch, television, book. You have the following items in your inventory: .\n",
      "On a scale of 1 to 10, that the action go to living room is relevant to the long-term goal is: 10\n"
     ]
    }
   ],
   "source": [
    "x = agent.generate_constrained_response(\"You are an intelligent robot. Your goal is to drop a knife in the living room. Knife is in the kitchen. You can navigate the environment, pick up items, and drop them. You are in the living room. You see: couch, television, book. You have the following items in your inventory: .\", [\"go to bedroom\", \"go to kitchen\", \"go to living room\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_without_prompt = x[len(prompt):].strip().split('\\n')[0][2:]\n",
    "print(response_without_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
